---
layout: post
title: "Cross entropy loss function"
author: "jungjik.lee"
categories: article
tags: [cross entropy]
---

**Cross-Entropy 손실 함수**는 분류 문제(특히 다중 클래스 분류)에서 자주 사용되는 손실 함수입니다. 예측된 확률 분포와 실제 정답(라벨) 분포 간의 차이를 측정합니다. **예측이 정답과 얼마나 일치하는지를 정량적으로 평가**하는 지표로 사용됩니다.

---

### 📘 **Cross-Entropy 손실 함수의 정의**
Cross-Entropy 손실 함수는 두 확률 분포 $\( p \)$ (정답 분포)와 $\( q \)$ (모델의 예측 분포) 사이의 차이를 측정합니다.

**수식 (다중 클래스 분류의 경우)**
$\[
L = - \sum_{i=1}^{n} y_i \log(\hat{y}_i)
\]$
- $\( L \)$: 손실 값 (Loss)
- $\( n \)$: 클래스의 개수
- $\( y_i \)$: 실제 정답 (정답 레이블, 원-핫 인코딩의 경우 0 또는 1)
- $\( \hat{y}_i \)$: 모델이 예측한 확률 값 (Softmax 함수의 출력)

---

### 📘 **수식의 의미**
1. **정답 레이블 ($\( y \)$)**: 실제 정답 레이블은 **원-핫 벡터**로 표현됩니다. 예를 들어, 클래스가 3개이고, 정답이 클래스 2라면 $\([0, 1, 0]\)$처럼 표현됩니다.
2. **모델의 예측 ($\( \hat{y} \)$)**: 모델은 Softmax 출력을 통해 각 클래스에 속할 확률을 예측합니다. 예를 들어, $\([0.1, 0.7, 0.2]\)$와 같이 확률이 출력됩니다.
3. **로그 함수 ($\( \log(\hat{y}_i) \)$)**: 모델이 정답 클래스의 확률을 높일수록 $\(\log(\hat{y}_i)\)$ 값은 커지므로 손실 값이 작아집니다.

---

### 📘 **예시**
#### 예를 들어, 3개의 클래스가 있는 다중 클래스 분류 문제에서:
- **정답 레이블 (y)**: $\([0, 1, 0]\)$ (두 번째 클래스가 정답)
- **모델의 예측 (ŷ)**: $\([0.1, 0.7, 0.2]\)$ (모델은 두 번째 클래스일 확률을 0.7로 예측)

손실을 계산합니다.
$\[
L = - \left( 0 \cdot \log(0.1) + 1 \cdot \log(0.7) + 0 \cdot \log(0.2) \right)
\]
\[
L = - \log(0.7) \approx - (-0.3567) \approx 0.3567
\]$
이 값이 **Cross-Entropy 손실**입니다.

---

### 📘 **왜 로그(log)를 사용할까?**
- **확률 해석**: 확률이 1에 가까울수록 $\(\log\)$ 값은 0에 가까워지고, 확률이 0에 가까울수록 $\(\log\)$ 값은 음의 무한대로 발산합니다. 이를 통해 모델의 예측이 정답에 더 가까울수록 손실이 작아지는 효과를 얻을 수 있습니다.
- **패널티 부여**: 정답 확률이 낮아질수록 더 큰 패널티(큰 손실 값)를 부여합니다. 예를 들어, 0.9와 0.1의 차이는 미미하지만, 0.01과 0.001의 차이는 훨씬 더 큰 영향을 미칩니다.

---

### 📘 **Cross-Entropy 손실 함수의 특성**
1. **확률 분포 비교**: 정답(라벨)과 예측(Softmax 출력) 간의 차이를 측정합니다.
2. **원-핫 벡터와의 관계**: 원-핫 벡터에서는 정답에 해당하는 부분만 남고 나머지는 모두 0이므로, 손실 함수는 정답 클래스에 대한 로그 손실로 단순화됩니다.
3. **확률의 합이 1**: Softmax 출력의 확률 합이 1이므로, 한 클래스의 확률이 높아질수록 다른 클래스의 확률은 낮아져야 합니다.

---

### 📘 **Cross-Entropy와 MSE(평균 제곱 오차) 비교**

| 구분 | Cross-Entropy | MSE (Mean Squared Error) |
| --- | :---: | :---: |
| **사용 용도** | 분류 문제 (특히 다중 클래스 분류) | 회귀 문제 (연속형 출력) |
| **수식 복잡도** | $\(- \sum y_i \log(\hat{y}_i)\)$ | $\(\frac{1}{n} \sum (y_i - \hat{y}_i)^2\)$ |
| **확률 해석** | 확률 분포의 차이를 비교 | 예측 값과 실제 값의 차이를 비교 |
| **로그 사용** | $\(\log(\hat{y}_i)\)$ 사용 | 로그 없음 |
| **오류 패널티** | 정답에 가까울수록 손실 감소 | 예측 값의 차이에 비례한 손실 |
| **출력 해석** | 확률로 해석 가능 | 값의 차이로 해석 |

---

### 📘 **왜 Cross-Entropy를 사용할까?**
1. **정확한 확률 예측**: Cross-Entropy는 예측 확률 분포와 정답 확률 분포를 비교하므로, 모델이 예측한 확률이 정답에 가까울수록 손실이 줄어듭니다.
2. **학습 속도 향상**: MSE에 비해 Gradient Descent의 속도가 빠르고 더 좋은 결과를 보이는 경우가 많습니다.
3. **확률 해석 가능**: 예측을 확률로 해석할 수 있기 때문에 모델의 신뢰성을 직관적으로 이해할 수 있습니다.

---

### 📘 **정리**
- Cross-Entropy 손실 함수는 **분류 문제에서 예측 확률과 정답 확률의 차이를 측정**합니다.
- 정답 레이블이 **원-핫 벡터**로 주어지면, 손실 값은 **정답 클래스에 해당하는 예측 확률의 로그**에 의해 결정됩니다.
- **Softmax 함수와 함께 사용**되며, 예측 클래스의 확률을 높이면 손실이 줄어들고, 틀린 클래스의 확률을 높이면 손실이 커집니다.

