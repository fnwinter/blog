---
layout: post
title: "하이퍼볼릭 탄젠트"
author: "jungjik.lee"
categories: article
tags: [Hyperbolic Tangent]
---

### 📘 **하이퍼볼릭 탄젠트 (Hyperbolic Tangent, tanh)란?**

**하이퍼볼릭 탄젠트(tanh)**는 수학에서 하이퍼볼릭 함수 중 하나로, 신경망(딥러닝)에서 **활성화 함수(activation function)**로 자주 사용됩니다. 

tanh 함수는 입력 값을 **-1에서 1 사이의 값으로 변환**합니다.  
이는 **시그모이드(sigmoid) 함수**와 유사하지만, 출력 범위가 \([-1, 1]\)로 더 넓습니다.

---

### 📌 **tanh 함수의 수식**
하이퍼볼릭 탄젠트 함수의 수식은 다음과 같습니다.

\[
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
\]

- \(e\)는 자연 상수(약 2.718)입니다.  
- x가 양수일 때, \(\tanh(x)\)는 1에 가까워지고,  
  x가 음수일 때, \(\tanh(x)\)는 -1에 가까워집니다.

---

### 📌 **tanh 함수의 그래프**
- \(x = 0\)일 때, \(\tanh(0) = 0\).  
- \(x \to \infty\)일 때, \(\tanh(x) \to 1\).  
- \(x \to -\infty\)일 때, \(\tanh(x) \to -1\).  

이로 인해, tanh 함수는 **S자 모양의 비선형 함수**로 그려집니다.  
다음은 tanh 함수의 특성입니다.

```
       1
       |
       |          .
       |      .
       |   .
y =    | .
       | 
       | .
       |   .
       |      .
       |          .
      -1-----------------
            x
```

---

### 📌 **tanh 함수의 특징**
| **특성**         | **설명**                                    |
|-----------------|---------------------------------------------|
| **출력 범위**     | \([-1, 1]\)                                |
| **중심(평균)**    | 0 (시그모이드의 평균 0.5와 다름)             |
| **미분 가능성**   | 모든 지점에서 미분 가능                     |
| **비선형성**      | 신경망의 비선형성을 부여함                  |
| **기울기 소실 문제**| x 값이 매우 크거나 작을 때, 기울기(gradient)가 0에 가까워지는 **기울기 소실 문제** 발생. |

---

### 📌 **tanh와 sigmoid의 차이**
| **특성**          | **tanh**                      | **sigmoid**                  |
|------------------|-------------------------------|-----------------------------|
| **출력 범위**      | \([-1, 1]\)                    | \([0, 1]\)                   |
| **평균 중심**      | 0                             | 0.5                          |
| **기울기 소실**     | 발생 (x가 클 때)               | 발생 (x가 클 때)             |
| **사용 위치**      | 은닉층 활성화 함수로 자주 사용  | 출력층에 자주 사용           |

- **tanh는 평균이 0**이기 때문에, 신경망의 **가중치 초기화가 더 효율적**일 수 있습니다.  
  (sigmoid의 경우 모든 값이 양수여서, 신경망의 학습 속도가 느려질 수 있습니다.)

---

### 📌 **tanh의 미분 (Gradient)**
tanh의 미분은 간단히 다음과 같이 계산할 수 있습니다.

\[
\frac{d}{dx} \tanh(x) = 1 - \tanh^2(x)
\]

이 미분 공식은 신경망의 **역전파(backpropagation)** 과정에서 사용됩니다.  
tanh의 값이 -1 또는 1에 가까워지면, 미분 값은 0에 가까워져 **기울기 소실(Gradient Vanishing)** 문제가 발생합니다.

---

### 📌 **tanh의 활용**
1. **신경망의 활성화 함수 (Activation Function)**
   - 은닉층의 활성화 함수로 사용됩니다.  
   - 이전에는 **시그모이드**를 많이 사용했지만, **tanh가 더 좋은 성능**을 보이는 경우가 많습니다.  
   - 현재는 **ReLU**가 더 많이 사용되지만, tanh는 여전히 일부 모델에서 사용됩니다.  

2. **신경망의 은닉 상태 (Hidden State)**
   - RNN, LSTM, GRU와 같은 순환 신경망(RNN)에서 **은닉 상태(hidden state)**의 값을 제한하기 위해 사용됩니다.  
   - LSTM 및 GRU의 내부 구조에서 **게이트의 출력**에 tanh가 자주 사용됩니다.  

3. **수학적 계산 및 모델링**
   - 물리학 및 공학에서는 하이퍼볼릭 함수가 물리적 시스템의 비선형 동작을 설명하는 데 사용됩니다.  

---

### 📌 **Python 코드 예시**

#### 1️⃣ **tanh 계산하기**
```python
import numpy as np
import matplotlib.pyplot as plt

# x 값 정의
x = np.linspace(-5, 5, 100)  # -5에서 5 사이의 100개 값
y = np.tanh(x)  # tanh 함수 적용

# 그래프 그리기
plt.plot(x, y, label="tanh(x)", color='orange')
plt.xlabel('x')
plt.ylabel('tanh(x)')
plt.title('Hyperbolic Tangent (tanh) Function')
plt.axhline(0, color='black',linewidth=0.5)  # y = 0 기준선
plt.axvline(0, color='black',linewidth=0.5)  # x = 0 기준선
plt.grid(True, linestyle='--', linewidth=0.5)
plt.legend()
plt.show()
```

이 코드는 **tanh 함수의 그래프**를 시각화합니다.  
S자 형태로, \(y = -1\)에서 \(y = 1\)로 수렴하는 그래프가 그려질 것입니다.

---

#### 2️⃣ **신경망에서 활성화 함수로 사용하기 (PyTorch)**
```python
import torch
import torch.nn as nn

# tanh 활성화 함수 사용 예시
x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])
tanh = nn.Tanh()  # PyTorch의 Tanh 함수
y = tanh(x)

print(f"입력값: {x}")
print(f"tanh 출력값: {y}")
```

**출력 결과**:
```
입력값: tensor([-2., -1.,  0.,  1.,  2.])
tanh 출력값: tensor([-0.9640, -0.7616,  0.0000,  0.7616,  0.9640])
```

tanh 함수가 x의 값에 따라 -1에서 1 사이의 값으로 변환되는 것을 확인할 수 있습니다.

---

### 📌 **정리**
1. **tanh**는 하이퍼볼릭 함수로, 입력을 **-1에서 1 사이**로 변환합니다.
2. **신경망의 활성화 함수**로 자주 사용되며, 특히 **LSTM, GRU**의 내부 게이트에서 자주 사용됩니다.
3. **시그모이드보다 평균이 0**에 가까워서 학습이 더 효율적입니다.
4. **미분 값**이 0에 가까워지는 경우, **기울기 소실(Gradient Vanishing)** 문제가 발생할 수 있습니다.
5. 현재는 **ReLU**가 더 자주 사용되지만, 일부 순환 신경망(RNN) 구조에서는 여전히 **tanh**를 사용합니다.

---