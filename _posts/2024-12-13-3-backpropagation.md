---
layout: post
title: "Backpropagation (μ—­μ „ν)"
author: "jungjik.lee"
categories: article
tags: [backpropagation]
---
μ—­μ „νμ—­μ „ν
**Backpropagation (μ—­μ „ν)**λ” μΈκ³µ μ‹ κ²½λ§(Artificial Neural Network, ANN)μ—μ„ **κ°€μ¤‘μΉ(weight)μ™€ νΈν–¥(bias)μ„ ν•™μµ**ν•κΈ° μ„ν•΄ μ‚¬μ©ν•λ” ν•µμ‹¬ μ•κ³ λ¦¬μ¦μ…λ‹λ‹¤. μ‹ κ²½λ§μ μ¶λ ¥ μ¤μ°¨λ¥Ό κ° μΈµμΌλ΅ **κ±°κΎΈλ΅ μ „ν**ν•λ©΄μ„ κ°€μ¤‘μΉλ¥Ό μ΅°μ •ν•λ” λ°©μ‹μΌλ΅, **κ²½μ‚¬ ν•κ°•λ²•(Gradient Descent)**μ μΌλ¶€λ¶„μΌλ΅ μ‘λ™ν•©λ‹λ‹¤.

---

## π“ **Backpropagationμ λ™μ‘ μ›λ¦¬**
Backpropagationμ ν•µμ‹¬ λ©ν‘λ” **μ†μ‹¤(loss) ν•¨μμ μ¤μ°¨λ¥Ό μµμ†ν™”**ν•κΈ° μ„ν•΄ **κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ΅°μ •**ν•λ” κ²ƒμ…λ‹λ‹¤. 

1. **μμ „ν (Forward Propagation)**
   - μ…λ ¥ λ°μ΄ν„°λ¥Ό λ°›μ•„ μ‹ κ²½λ§μ κ° μΈµμ„ ν†µκ³Όμ‹ν‚¤λ©΄μ„ **μ¶λ ¥(yΜ‚, μμΈ΅κ°’)μ„ κ³„μ‚°**ν•©λ‹λ‹¤.
   - μ΄ κ³Όμ •μ—μ„ κ°€μ¤‘μΉ(weight)μ™€ νΈν–¥(bias)μ΄ μ‚¬μ©λ©λ‹λ‹¤.
   - μµμΆ… μ¶λ ¥(yΜ‚)κ³Ό μ •λ‹µ(y) κ°„μ **μ†μ‹¤(loss) κ°’**μ„ κ³„μ‚°ν•©λ‹λ‹¤.

2. **μ†μ‹¤(loss) κ³„μ‚°**
   - μμΈ΅λ μ¶λ ¥κ³Ό μ‹¤μ  μ •λ‹µ μ‚¬μ΄μ μ°¨μ΄λ¥Ό μ†μ‹¤ ν•¨μ(μ: **Cross-Entropy** λλ” **MSE**)λ΅ μΈ΅μ •ν•©λ‹λ‹¤.
   
3. **μ¤μ°¨ μ—­μ „ν (Backward Propagation)**
   - **μ¶λ ¥μΈµμ—μ„ μ…λ ¥μΈµμΌλ΅ μ—­λ°©ν–¥μΌλ΅ μ¤μ°¨λ¥Ό μ „ν**ν•©λ‹λ‹¤.
   - λ―Έλ¶„(κΈ°μΈκΈ°, Gradient)μ„ ν†µν•΄ μ†μ‹¤ ν•¨μμ λ³€ν™”μ— λ€ν• κ°€μ¤‘μΉμ λ³€ν™”($\( \frac{\partial L}{\partial w} \)$)λ¥Ό κ³„μ‚°ν•©λ‹λ‹¤.

4. **κ°€μ¤‘μΉμ™€ νΈν–¥ μ—…λ°μ΄νΈ**
   - **κ²½μ‚¬ ν•κ°•λ²•(Gradient Descent)** λλ” **Adam Optimizer**μ™€ κ°™μ€ μµμ ν™” μ•κ³ λ¦¬μ¦μ„ μ‚¬μ©ν•μ—¬ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ—…λ°μ΄νΈν•©λ‹λ‹¤.
   $\[
   w_{\text{new}} = w_{\text{old}} - \eta \frac{\partial L}{\partial w}
   \]$
   - $\( \eta \)$λ” **ν•™μµλ¥ (learning rate)**μΌλ΅, ν• λ²μ μ—…λ°μ΄νΈμ—μ„ κ°€μ¤‘μΉλ¥Ό μ–Όλ§λ‚ μ΅°μ •ν• μ§€λ¥Ό κ²°μ •ν•©λ‹λ‹¤.

---

## π“ **μν•™μ  μ„¤λ…**
μ‹ κ²½λ§μ μ¶λ ¥ λ…Έλ“ $\( \hat{y} \)$μ— λ€ν• μ†μ‹¤ $\( L \)$μ„ μµμ†ν™”ν•λ ¤κ³  ν•  λ•, κ° κ°€μ¤‘μΉ $\( w \)$μ— λ€ν•΄ **μ†μ‹¤μ λ³€ν™”μ¨(κΈ°μΈκΈ°, gradient)**λ¥Ό κ³„μ‚°ν•©λ‹λ‹¤.

### **1οΈβƒ£ μμ „ν (Forward Propagation)**
- μ…λ ¥ \( x \)μ—μ„ μ‹μ‘ν•μ—¬ **κ°€μ¤‘μΉ \( w \)**, **ν™μ„±ν™” ν•¨μ \( f \)**λ¥Ό κ±°μ³ μµμΆ… μμΈ΅κ°’ $\( \hat{y} \)$λ¥Ό κµ¬ν•©λ‹λ‹¤.
$\[
a^{(l)} = f(z^{(l)}) \quad \text{(ν™μ„±ν™”κ°’)}
\]
\[
z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)} \quad \text{(κ°€μ¤‘ν•©)}
\]$

### **2οΈβƒ£ μ†μ‹¤(Loss) κ³„μ‚°**
- μμΈ΅κ°’ $\( \hat{y} \)$μ™€ μ‹¤μ κ°’ $\( y \)$μ μ†μ‹¤ $\( L \)$μ„ κ³„μ‚°ν•©λ‹λ‹¤.
$\[
L = - \sum_{i} y_i \log(\hat{y}_i)
\]$

### **3οΈβƒ£ μ—­μ „ν (Backward Propagation)**
#### **μ¶λ ¥μΈµμ λ―Έλ¶„**
μ¶λ ¥μΈµμ κ°€μ¤‘μΉμ— λ€ν• μ†μ‹¤μ λ³€ν™”μ¨ $\(\frac{\partial L}{\partial W}\)$λ¥Ό κµ¬ν•©λ‹λ‹¤.
$\[
\frac{\partial L}{\partial z^{(l)}} = \hat{y} - y
\]$
μ—¬κΈ°μ„ $\( \hat{y} \)$λ” μμΈ΅κ°’, $\( y \)$λ” μ‹¤μ κ°’μ…λ‹λ‹¤. 

#### **μ€λ‹‰μΈµμ λ―Έλ¶„**
μ€λ‹‰μΈµμ κ°€μ¤‘μΉμ— λ€ν• λ³€ν™”μ¨μ€ μ—°μ‡„ λ²•μΉ™(Chain Rule)μ„ μ‚¬μ©ν•©λ‹λ‹¤.
$\[
\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}}
\]$
μ—¬κΈ°μ„ $\(\frac{\partial z^{(l)}}{\partial W^{(l)}} = a^{(l-1)}\)$ μ΄κΈ° λ•λ¬Έμ—, μ€λ‹‰μΈµμ λ―Έλ¶„μ€ μ•„λμ™€ κ°™μ΄ ν‘ν„ν•  μ μμµλ‹λ‹¤.
$\[
\frac{\partial L}{\partial W^{(l)}} = \delta^{(l)} \cdot a^{(l-1)} 
\]$
μ—¬κΈ°μ„ $\(\delta^{(l)}\)$λ” μ¤μ°¨ μ‹ νΈ(error signal)λ΅, λ‹¤μ μΈµμΌλ΅ μ „νλ©λ‹λ‹¤.

#### **νΈν–¥μ λ―Έλ¶„**
νΈν–¥μ— λ€ν• λ―Έλ¶„λ„ κ°€μ¤‘μΉμ™€ μ μ‚¬ν•κ² κ³„μ‚°λλ©°, νΈν–¥μ€ λ¨λ“  μ…λ ¥μ— λ€ν•΄ λ™μΌν•κ² μ μ©λλ―€λ΅ λ‹¨μν λ„μ ν•©λ‹λ‹¤.
$\[
\frac{\partial L}{\partial b^{(l)}} = \delta^{(l)} 
\]$

---

## π“ **μ—°μ‡„ λ²•μΉ™ (Chain Rule)**
Backpropagationμ ν•µμ‹¬μ€ **μ—°μ‡„ λ²•μΉ™(Chain Rule)**μ„ ν™μ©ν•΄ μ†μ‹¤ ν•¨μμ— λ€ν• κ° κ°€μ¤‘μΉμ κΈ°μΈκΈ°λ¥Ό κ³„μ‚°ν•λ” κ²ƒμ…λ‹λ‹¤. μ—°μ‡„ λ²•μΉ™μ κΈ°λ³Έ κ°λ…μ€ λ‹¤μκ³Ό κ°™μµλ‹λ‹¤.

$\[
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w}
\]$

---

## π“ **Backpropagation μμ‹**
κ°„λ‹¨ν• 2μΈµ μ‹ κ²½λ§μ΄ μλ‹¤κ³  κ°€μ •ν•©λ‹λ‹¤.

1. **μμ „ν:**
   - μ…λ ¥ $\( x = 0.5 \)$
   - κ°€μ¤‘μΉ $\( w = 0.8 \)$
   - μ¶λ ¥ $\( z = w \cdot x = 0.8 \cdot 0.5 = 0.4 \)$
   - ν™μ„±ν™” ν•¨μ $\( f(z) \)$λ΅ Sigmoid ν•¨μ μ μ©
   $\[
   f(0.4) = \frac{1}{1 + e^{-0.4}} \approx 0.5987
   \]$

2. **μ†μ‹¤ κ³„μ‚°:**
   - μ •λ‹µ λ μ΄λΈ” $\( y = 1 \)$ (μ›ν•« μΈμ½”λ”©)
   - μ†μ‹¤ $\( L = -y \cdot \log(\hat{y}) = -1 \cdot \log(0.5987) \approx 0.513 \)$

3. **μ—­μ „ν:**
   - **μ¶λ ¥μΈµ**: μ¤μ°¨ κ³„μ‚° $\(\frac{\partial L}{\partial z} = \hat{y} - y = 0.5987 - 1 = -0.4013\)$
   - **μ€λ‹‰μΈµ**: κ°€μ¤‘μΉμ λ³€ν™”μ¨ $\(\frac{\partial L}{\partial w} = \frac{\partial L}{\partial z} \cdot \frac{\partial z}{\partial w} = -0.4013 \cdot 0.5 = -0.2007\)$

---

## π“ **Why Backpropagation? (μ™ μ‚¬μ©ν•λ”κ°€?)**
1. **ν¨μ¨μ μΈ ν•™μµ**: μ—°μ‡„ λ²•μΉ™μ„ μ‚¬μ©ν•μ—¬ κ³„μ‚° λΉ„μ©μ„ μ¤„μ΄κΈ° λ•λ¬Έμ— μ‹ κ²½λ§ ν•™μµμ„ ν¨μ¨μ μΌλ΅ μν–‰ν•  μ μμµλ‹λ‹¤.
2. **λ¨λ“  κ°€μ¤‘μΉ μ—…λ°μ΄νΈ κ°€λ¥**: μ—¬λ¬ μΈµμ κ°€μ¤‘μΉκ°€ λ™μ‹μ— μ—…λ°μ΄νΈλμ–΄ μ‹ κ²½λ§μ΄ ν•™μµν•©λ‹λ‹¤.
3. **μλ™ λ―Έλ¶„ (Autodiff)**: TensorFlowμ™€ PyTorchλ” Backpropagation μ•κ³ λ¦¬μ¦μ„ μλ™μΌλ΅ μν–‰ν•©λ‹λ‹¤.

---

## π“ **μ”μ•½**
- **Backpropagation**μ€ μ‹ κ²½λ§μ—μ„ **μ¤μ°¨λ¥Ό κ±°κΎΈλ΅ μ „ν**ν•μ—¬ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ΅°μ •ν•λ” κ³Όμ •μ…λ‹λ‹¤.
- **μμ „ν**λ¥Ό ν†µν•΄ μμΈ΅κ°’μ„ κ³„μ‚°ν•κ³ , **μ†μ‹¤ ν•¨μμ κΈ°μΈκΈ°(gradient)λ¥Ό κ³„μ‚°**ν•μ—¬ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ—…λ°μ΄νΈν•©λ‹λ‹¤.
- **μ—°μ‡„ λ²•μΉ™(Chain Rule)**μ„ ν†µν•΄ κ° μΈµμ λ³€ν™”μ¨μ„ ν¨μ¨μ μΌλ΅ κ³„μ‚°ν•©λ‹λ‹¤.
- **κ²½μ‚¬ ν•κ°•λ²•**μ„ μ‚¬μ©ν•΄ κ°€μ¤‘μΉμ™€ νΈν–¥μ„ μ—…λ°μ΄νΈν•μ—¬ ν•™μµμ΄ μ΄λ£¨μ–΄μ§‘λ‹λ‹¤.

---
