---
layout: post
title: "Softmax"
author: "jungjik.lee"
categories: article
tags: [softmax]
---

**Softmax 함수**는 기계 학습 및 딥러닝에서 자주 사용되는 **확률 분포 함수**입니다. 주로 **분류 문제**에서 각 클래스에 속할 확률을 예측할 때 사용됩니다.

---

### 📘 **Softmax 함수의 정의**
Softmax 함수는 입력 벡터 $\( z = [z_1, z_2, \cdots, z_n] \)$에 대해 각 요소 $\( z_i \)$를 **확률 값**으로 변환합니다. 이 확률 값들은 모두 0과 1 사이에 있으며, 모든 확률의 합은 1이 됩니다. 

**수식:**
$\[\sigma(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{n} e^{z_j}}\]$
- $\( z_i \)$: 입력 벡터의 $$\( i \)번째 원소
- $\( e \)$: 자연상수 (약 2.718)
- $\( n \)$: 벡터의 원소 개수

---

### 📘 **Softmax 함수의 동작 원리**
1. **지수화 (Exponentiation)**: 각 입력 \( z_i \)에 대해 \( e^{z_i} \)를 계산합니다. 이 과정에서 입력값이 더 크면 더 큰 가중치를 갖게 됩니다.
2. **정규화 (Normalization)**: 지수화한 값의 합으로 나누어 각 확률을 계산합니다. 이로 인해 확률의 총합이 1이 됩니다.

---

### 📘 **Softmax 함수의 예시**
입력 벡터 $\( z = [2, 1, 0] \)$가 주어졌을 때, 각 원소에 대해 Softmax 함수를 적용합니다.

1. **지수화**: 
   $\[
   e^2 \approx 7.389, \quad e^1 \approx 2.718, \quad e^0 = 1
   \]$
   
2. **정규화**: 
   $\[
   \sigma(z_1) = \frac{7.389}{7.389 + 2.718 + 1} \approx 0.659
   \]
   \[
   \sigma(z_2) = \frac{2.718}{7.389 + 2.718 + 1} \approx 0.242
   \]
   \[
   \sigma(z_3) = \frac{1}{7.389 + 2.718 + 1} \approx 0.089
   \]$
   최종 확률 벡터는 \([0.659, 0.242, 0.089]\)입니다.

---

### 📘 **Softmax의 활용**
1. **분류 문제**: 신경망의 마지막 층에 Softmax를 적용하여 각 클래스에 속할 확률을 예측합니다.
2. **다중 클래스 분류**: 로지스틱 회귀의 확장판으로, 하나의 입력이 여러 개의 클래스 중 하나에 속할 확률을 반환합니다.

---

### 📘 **왜 Softmax를 사용할까?**
- **확률 분포 생성**: 예측 값들이 확률로 변환되기 때문에, 분류 문제의 직관적인 해석이 가능합니다.
- **Gradient 계산이 용이**: 역전파(Backpropagation) 과정에서 미분이 가능하여 신경망 학습에 적합합니다.

---
